{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pyodbc\n",
    "import logging\n",
    "import requests\n",
    "import copy\n",
    "import math\n",
    "import time\n",
    "import nltk\n",
    "from elasticsearch import helpers\n",
    "\n",
    "from elasticsearch import Elasticsearch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_connection(db_config_path):\n",
    "    with open(db_config_path, encoding='utf8') as db:\n",
    "        database = ';'.join(db.readlines()[1:]).replace('\\n', '').replace(' ', '') + ';'\n",
    "    return pyodbc.connect('DRIVER={/opt/microsoft/msodbcsql18/lib64/libmsodbcsql-18.3.so.2.1};' + database + 'TrustServerCertificate=yes;')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "con = get_connection('materiaisServicos/config.ini')\n",
    "sqlMaterial = \"\"\"SELECT \n",
    "            m.ID_MATERIAL, \n",
    "            p.TIPO, \n",
    "            M.CODIGO_MATERIAL, \n",
    "            M.ITEM_SUSPENSO,\n",
    "            M.MATERIAL_SUSTENTAVEL,\n",
    "            c.CODIGO_CLASSE,\n",
    "            c.DESCRICAO_CLASSE,\n",
    "            s.CODIGO_SUBCLASSE,\n",
    "            s.DESCRICAO_SUBCLASSE,\n",
    "            g.CODIGO_GRUPO,\n",
    "            g.DESCRICAO_GRUPO,\n",
    "            d.CODIGO_DIVISAO,\n",
    "            d.DESCRICAO_DIVISAO,\n",
    "            s2.CODIGO_SECAO,\n",
    "            s2.DESCRICAO_SECAO,\n",
    "            ISNULL(STUFF((SELECT ' ' + CONCAT(c.CARACTERISTICA, ' ', CV.CARACTERISTICA_VALOR, cv.SIGLA_UNIDADE_MEDIDA)\n",
    "                        FROM CATALOGO.MATERIAL_CARACTERISTICA mc\n",
    "                        JOIN CATALOGO.PDM_CARACTERISTICA pc ON pc.ID_PDM_CARACTERISTICA = mc.ID_PDM_CARACTERISTICA\n",
    "                        JOIN CATALOGO.CARACTERISTICAS c ON pc.ID_CARACTERISTICA = c.ID_CARACTERISTICA\n",
    "                        JOIN CATALOGO.CARACTERISTICAS_VALORES cv ON cv.ID_CARACTERISTICA_VALOR = pc.ID_CARACTERISTICA_VALOR\n",
    "                        WHERE mc.ID_MATERIAL = m.ID_MATERIAL\n",
    "                        FOR XML PATH('')), 1, 1, ''), '') AS CARACTERISTICAS_BUSCA,\n",
    "            p.NOME_MATERIAL, \n",
    "            CONCAT(p.NOME_MATERIAL, ' ', ISNULL(STUFF((SELECT ' ' + CONCAT(c.CARACTERISTICA, ' ', CV.CARACTERISTICA_VALOR, cv.SIGLA_UNIDADE_MEDIDA)\n",
    "                                                    FROM CATALOGO.MATERIAL_CARACTERISTICA mc\n",
    "                                                    JOIN CATALOGO.PDM_CARACTERISTICA pc ON pc.ID_PDM_CARACTERISTICA = mc.ID_PDM_CARACTERISTICA\n",
    "                                                    JOIN CATALOGO.CARACTERISTICAS c ON pc.ID_CARACTERISTICA = c.ID_CARACTERISTICA\n",
    "                                                    JOIN CATALOGO.CARACTERISTICAS_VALORES cv ON cv.ID_CARACTERISTICA_VALOR = pc.ID_CARACTERISTICA_VALOR\n",
    "                                                    WHERE mc.ID_MATERIAL = m.ID_MATERIAL\n",
    "                                                    FOR XML PATH('')), 1, 1, ''), '')) AS MATERIAL_TEXT\n",
    "        FROM CATALOGO.MATERIAIS m\n",
    "        JOIN CATALOGO.PDM p ON m.ID_PDM = p.ID_PDM\n",
    "        LEFT JOIN CATALOGO.CLASSES c ON p.ID_CLASSE = c.ID_CLASSE\n",
    "        LEFT JOIN CATALOGO.SUBCLASSES s ON c.ID_CLASSE = s.ID_CLASSE\n",
    "        LEFT JOIN CATALOGO.GRUPOS g ON g.ID_GRUPO = c.ID_GRUPO\n",
    "        LEFT JOIN CATALOGO.DIVISOES d ON d.ID_DIVISAO = g.ID_DIVISAO\n",
    "        LEFT JOIN CATALOGO.SECOES s2 ON S2.ID_SECAO = D.ID_SECAO\n",
    "        WHERE p.TIPO = 'M'\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_materiais = pd.read_sql_query(sqlMaterial, con)\n",
    "df_materiais"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_materiais = pd.read_csv('df_materiais.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes_count = df_materiais['DESCRICAO_CLASSE'].value_counts()\n",
    "classes_count = classes_count[classes_count < 100]\n",
    "classes_count.index.tolist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes_count = classes_count[classes_count > 5]\n",
    "classes_count = classes_count.index.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_range_sample = df_materiais[df_materiais['DESCRICAO_CLASSE'].isin(classes_count)]\n",
    "df_range_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_catmat = df_range_sample.sample(50)\n",
    "sample_catmat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_catmat['MATERIAL_TEXT']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_catmat = pd.read_csv('result_resample.csv')\n",
    "sample_catmat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "itens_a_retirar = sample_catmat['ID_MATERIAL']\n",
    "itens_a_retirar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_index_catmat = df_materiais[~df_materiais['ID_MATERIAL'].isin(itens_a_retirar)]\n",
    "df_index_catmat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_catmat = pd.read_csv('sample_catmat.csv')\n",
    "sample_catmat\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_index_catmat = pd.read_csv('df_index_catmat_resample.csv')\n",
    "df_index_catmat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "descricoes_catmat = sample_catmat['MATERIAL_TEXT']\n",
    "descricoes_catmat.to_csv('descricoes_sample_catmat.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_catmat['MATERIAL_TEXT']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dados para indexação\n",
    "\n",
    "Retirando dados utilizados como teste de relevância do df a ser indexado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_index_catmat[df_index_catmat['MATERIAL_TEXT'].str.contains('Nome Agenda')][['ID_MATERIAL', 'MATERIAL_TEXT']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_index_catmat.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RESAMPLE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_catmat = pd.read_csv('result_resample.csv')\n",
    "sample_catmat\n",
    "new_sampl = df_materiais[df_materiais['ID_MATERIAL'].isin([581197, 508151])]\n",
    "new_sampl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_catmat = sample_catmat[~sample_catmat['ID_MATERIAL'].isin([508072, 581205])]\n",
    "sample_catmat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_catmat = pd.concat([new_sampl, sample_catmat], ignore_index=True)\n",
    "sample_catmat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_catmat['relevantes'] = None\n",
    "for idx, query in sample_catmat.iterrows():\n",
    "    nome_mater = query['NOME_MATERIAL']\n",
    "    results_descricao = df_index_catmat[df_index_catmat['NOME_MATERIAL'] == nome_mater]\n",
    "    sample_catmat['relevantes'][idx] = [(row['ID_MATERIAL'], 2) for _, row in results_descricao.iterrows()]\n",
    "sample_catmat = sample_catmat[['ID_MATERIAL', 'NOME_MATERIAL', 'MATERIAL_TEXT', 'relevantes']]\n",
    "sample_catmat\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Realizando indexação lexica\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "es = Elasticsearch(\n",
    "    \"http://localhost:9202\",\n",
    "    basic_auth=('elastic', 'teste123'),\n",
    "    request_timeout=999999999,\n",
    ")\n",
    "\n",
    "es.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def doc_generator_lex(index_name, df):\n",
    "  for idx, row in df.iterrows():\n",
    "    yield {\n",
    "        \"_index\": index_name,\n",
    "        \"_id\": idx,\n",
    "        \"_source\": {\n",
    "           \"id_material\": row['ID_MATERIAL'],\n",
    "            \"codigo_classe\": row['CODIGO_CLASSE'],\n",
    "            \"descricao_classe\": row['DESCRICAO_CLASSE'],\n",
    "            \"codigo_grupo\": row['CODIGO_GRUPO'],\n",
    "            \"caracteristica_busca\": row['CARACTERISTICAS_BUSCA'],\n",
    "            \"nome_material\": row['NOME_MATERIAL'],\n",
    "            \"material_text\": row['MATERIAL_TEXT'],\n",
    "        }\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices = es.cat.indices(h='index', format='json')\n",
    "nomes_indices = [indice['index'] for indice in indices]\n",
    "nomes_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "INDEX_NAME = \"matlex\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "documentos_notas = doc_generator_lex(INDEX_NAME, df_index_catmat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "es.options(ignore_status=[400,404]).indices.delete(index=INDEX_NAME)\n",
    "\n",
    "body = {\n",
    "   \"settings\": {\n",
    "        'number_of_shards': 3,\n",
    "        'number_of_replicas': 1,\n",
    "        \"analysis\": {\n",
    "            \"analyzer\": {\n",
    "                \"standard_asciifolding\": {\n",
    "                    \"type\": \"custom\",\n",
    "                    \"tokenizer\": \"standard\",\n",
    "                    \"filter\": [ \"lowercase\", \"asciifolding\" ]\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    },\n",
    "    \n",
    "    'mappings': {\n",
    "        'properties': {\n",
    "            'id_material': {'type': 'long' },\n",
    "            'codigo_classe': {'type': 'keyword' },\n",
    "            'descricao_classe': {'type': 'text' },\n",
    "            'codigo_grupo': {'type': 'keyword' },\n",
    "            'caracteristica_busca': {'type': 'text' },\n",
    "            'nome_material': {'type': 'text' },\n",
    "            'material_text': {'type': 'text' },\n",
    "        }\n",
    "    }\n",
    "\n",
    "}\n",
    "\n",
    "es.indices.create(index=INDEX_NAME, body=body)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices = es.cat.indices(h='index', format='json')\n",
    "nomes_indices = [indice['index'] for indice in indices]\n",
    "nomes_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    resp = helpers.bulk(es, documentos_notas)\n",
    "\n",
    "    if resp[1]:\n",
    "        errors = resp[1]\n",
    "        for error in errors:\n",
    "            print(\"Falha de indexação:\", error)\n",
    "except helpers.BulkIndexError as e:\n",
    "    for err in e.errors:\n",
    "        print(\"Falha de indexação:\", err)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Teste de query a indexação lexica\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = {\n",
    "    \"query\": {\n",
    "        \"match_all\": {}\n",
    "    },\n",
    "}\n",
    "\n",
    "resp = es.search(index=INDEX_NAME, body=query)\n",
    "print(\"Got %d Hits:\" % resp['hits']['total']['value'])\n",
    "for hit in resp['hits']['hits']:\n",
    "    print(hit[\"_source\"]['material_text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Realizando indexação Semantica"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "modelL6V2 = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "modelBV2 = SentenceTransformer('sentence-transformers/all-mpnet-base-v2')\n",
    "\n",
    "modelMult = SentenceTransformer('sentence-transformers/quora-distilbert-multilingual')\n",
    "modelLASE = SentenceTransformer('sentence-transformers/LaBSE')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df_index_catmat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_index_catmat['DESCRICAO_CLASSE'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# embeddings = modelL6V2.encode(df_materiais['MATERIAL_TEXT'].tolist(), show_progress_bar=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# embeddings = modelL6V2.encode(df_index_catmat['MATERIAL_TEXT'].tolist(), show_progress_bar=True)\n",
    "embeddingsL6V2 = modelL6V2.encode(df_index_catmat['MATERIAL_TEXT'].tolist(), show_progress_bar=False)\n",
    "print('modelL6V2 terminado')\n",
    "embeddingsBV2 = modelBV2.encode(df_index_catmat['MATERIAL_TEXT'].tolist(), show_progress_bar=False)\n",
    "print('modelBV2 terminado')\n",
    "embeddingsMult = modelMult.encode(df_index_catmat['MATERIAL_TEXT'].tolist(), show_progress_bar=False)\n",
    "print('modelMult terminado')\n",
    "embeddingsLASE = modelLASE.encode(df_index_catmat['MATERIAL_TEXT'].tolist(), show_progress_bar=False)\n",
    "print('modelLASE terminado')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddingsL6V2[0]\n",
    "embeddingsBV2[0]\n",
    "embeddingsMult[0]\n",
    "embeddingsLASE[244337]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def doc_generator_sm(index_name, df):\n",
    "  for idx, row in df.iterrows():\n",
    "    yield {\n",
    "        \"_index\": index_name,\n",
    "        \"_id\": idx,\n",
    "        \"_source\": {\n",
    "            \"id_material\": row['ID_MATERIAL'],\n",
    "            \"codigo_classe\": row['CODIGO_CLASSE'],\n",
    "            \"descricao_classe\": row['DESCRICAO_CLASSE'],\n",
    "            \"codigo_grupo\": row['CODIGO_GRUPO'],\n",
    "            \"caracteristica_busca\": row['CARACTERISTICAS_BUSCA'],\n",
    "            \"nome_material\": row['NOME_MATERIAL'],\n",
    "            \"material_text\": row['MATERIAL_TEXT'],\n",
    "            \"all_mini_base_vector\": embeddingsL6V2[idx],\n",
    "            \"bv2_base_vector\": embeddingsBV2[idx],\n",
    "            \"mpnet_base_vector\": embeddingsMult[idx],\n",
    "            \"labase_base_vector\": embeddingsLASE[idx]\n",
    "        }\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SMNT = \"mat_semaanc\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ctm_doc = doc_generator_sm(SMNT, df_index_catmat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "es.options(ignore_status=[400,404]).indices.delete(index=SMNT)\n",
    "\n",
    "body = {\n",
    "    \"settings\": {\n",
    "        'number_of_shards': 3,\n",
    "        'number_of_replicas': 1,\n",
    "        \"analysis\": {\n",
    "            \"analyzer\": {\n",
    "                \"standard_asciifolding\": {\n",
    "                    \"type\": \"custom\",\n",
    "                    \"tokenizer\": \"standard\",\n",
    "                    \"filter\": [ \"lowercase\", \"asciifolding\" ]\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    },\n",
    "    \n",
    "    'mappings': {\n",
    "        'properties': {\n",
    "            'id_material': {'type': 'long' },\n",
    "            'codigo_classe': {'type': 'keyword' },\n",
    "            'descricao_classe': {'type': 'text' },\n",
    "            'codigo_grupo': {'type': 'keyword' },\n",
    "            'caracteristica_busca': {'type': 'text' },\n",
    "            'nome_material': {'type': 'text' },\n",
    "            'material_text': {'type': 'text' },\n",
    "            'all_mini_base_vector': {\n",
    "                'type': 'dense_vector',\n",
    "                'dims': 384,\n",
    "                'index': True,\n",
    "                'similarity': 'cosine'\n",
    "            },\n",
    "            'bv2_base_vector': {\n",
    "                'type': 'dense_vector',\n",
    "                'dims': 768,\n",
    "                'index': True,\n",
    "                'similarity': 'cosine'\n",
    "            },\n",
    "            'mpnet_base_vector': {\n",
    "                'type': 'dense_vector',\n",
    "                'dims': 768,\n",
    "                'index': True,\n",
    "                'similarity': 'cosine'\n",
    "            },\n",
    "            'labase_base_vector': {\n",
    "                'type': 'dense_vector',\n",
    "                'dims': 768,\n",
    "                'index': True,\n",
    "                'similarity': 'cosine'\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "\n",
    "}\n",
    "\n",
    "es.indices.create(index=SMNT, body=body)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices = es.cat.indices(h='index', format='json')\n",
    "nomes_indices = [indice['index'] for indice in indices]\n",
    "nomes_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tamanho_lote = 100\n",
    "dcmen = list(ctm_doc)\n",
    "num_batches = (len(dcmen) + tamanho_lote - 1) // tamanho_lote \n",
    "\n",
    "batches = [dcmen[i * tamanho_lote:(i + 1) * tamanho_lote] for i in range(num_batches - 1)] \n",
    "if len(dcmen) % tamanho_lote <= tamanho_lote:\n",
    "    ultimo_batch = dcmen[(num_batches - 1) * tamanho_lote:]\n",
    "    batches.append(ultimo_batch)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import traceback\n",
    "\n",
    "for batch in batches:\n",
    "    \n",
    "    try:\n",
    "        response = helpers.bulk(es, batch)\n",
    "\n",
    "        if response[1]:\n",
    "            errors = response[1]\n",
    "            for error in errors:\n",
    "                print(\"Falha de indexação:\", error)\n",
    "    except helpers.BulkIndexError as e:\n",
    "        for err in e.errors:\n",
    "            print(\"Falha de indexação:\", err)\n",
    "        traceback.print_exc()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_embedding = modelBV2.encode(\"Bandeja Revelação Material Plástico Características Adicionais Ranhuras Fundo E Apoio Para Termômetro Altura 7L/MIN Comprimento 18POL Largura 13 Cor Bege\")\n",
    "\n",
    "query_smt= {\n",
    "    'field': 'bv2_base_vector',\n",
    "    'query_vector': query_embedding.tolist(),\n",
    "    'k': 10,\n",
    "    'num_candidates': 10\n",
    "}\n",
    "\n",
    "\n",
    "results = es.search(index=SMNT, knn=query_smt, source=['descricao_produto'])\n",
    "\n",
    "results = [\n",
    "    {\n",
    "        \"_score\": hit[\"_score\"],\n",
    "        \"_source\": hit[\"_source\"]\n",
    "    }\n",
    "    for hit in results[\"hits\"][\"hits\"]\n",
    "]\n",
    "\n",
    "for hit in results:\n",
    "    print(\"Score:\", hit[\"_score\"])\n",
    "    print(\"Source:\", hit[\"_source\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Realizando mapeamento de relevância"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dv_relev_novo = dv_relev_sample.reset_index(drop =True)\n",
    "dv_relev_novo['CODIGO_BARRA']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ruidos = ['query', 'sinonimos', 'erros_digitacao', 'reordenados']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_catmat = pd.read_csv('sample_catmat_test.csv')\n",
    "sample_catmat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_catmat = pd.read_csv('resample_test.csv')\n",
    "sample_catmat['relevantes'] = None\n",
    "for idx, query in sample_catmat.iterrows():\n",
    "    nome_mater = query['NOME_MATERIAL']\n",
    "    results_descricao = df_index_catmat[df_index_catmat['NOME_MATERIAL'] == nome_mater]\n",
    "    sample_catmat['relevantes'][idx] = [(row['ID_MATERIAL'], 2) for _, row in results_descricao.iterrows()]\n",
    "sample_catmat = sample_catmat[['ID_MATERIAL', 'NOME_MATERIAL', 'MATERIAL_TEXT', 'relevantes']]\n",
    "sample_catmat\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "queries = []\n",
    "for idx, row in sample_catmat.iterrows():\n",
    "    queries.append({'idx':idx,'query': row['MATERIAL_TEXT'], 'page_size': 100})\n",
    "\n",
    "result_queries = []\n",
    "for q in queries:\n",
    "     q_elastic = copy.deepcopy(q)\n",
    "     q_elastic['abordagem'] = 'lexico'\n",
    "     result_queries.append(q_elastic)\n",
    "     q_sql = copy.deepcopy(q)\n",
    "     q_sql['abordagem'] = 'semantico'\n",
    "     result_queries.append(q_sql)\n",
    "\n",
    "df_queries_relv = pd.DataFrame(result_queries)\n",
    "df_queries_relv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_erros_dig = pd.read_csv('erros_digitacao_resample.csv')\n",
    "# df_sinonimos = pd.read_csv('sinonimos_resample.csv')\n",
    "df_termos_reord = pd.read_csv('reordenados_resample.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!export GROQ_API_KEY=\"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\"\n",
    "from groq import Groq\n",
    "import os\n",
    "os.environ['GROQ_API_KEY'] = \"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\"\n",
    "api_key = os.environ.get(\"GROQ_API_KEY\")\n",
    "print(f\"GROQ_API_KEY: {api_key}\")\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = Groq(api_key=os.environ.get(\"GROQ_API_KEY\"))\n",
    "\n",
    "def generate_synonym_descriptions(df):\n",
    "    instructions = \"\"\"\n",
    "    ### Instruções:\n",
    "    Abaixo você receberá descrições de produtos. Para cada descrição, gere apenas uma variação que se refere ao mesmo produto, respeitando os valores como altura, profundidade e volume. As unidades de medida podem ser substituídas por sua nomenclatura por extenso (por exemplo, cm para \"centímetros\").\n",
    "\n",
    "    ### Exemplos:\n",
    "\n",
    "    #### Exemplo 1:\n",
    "    #### Descrição original:\n",
    "    Forno Mufla Aplicação Tratamento Térmico Temperatura Máxima 1.400V Largura 600MBPS Altura 400V Profundidade 680MMHG Volume 163MM\n",
    "\n",
    "    #### Variações:\n",
    "    1. Forno Mufla para Tratamento Térmico com Temperatura Máxima de 1.400 Volts, Largura de 600 Megabits por Segundo, Altura de 400 Volts, Profundidade de 680 Milímetros de Mercúrio e Volume de 163 Milímetros\n",
    "    2. Forno Mufla para Processos Térmicos, alcançando até 1.400V, com Largura de 600MBPS, Altura de 400V, Profundidade de 680MMHG e Volume de 163MM\n",
    "    3. Forno Mufla adequado para Tratamento Térmico com Temperatura Máxima de 1.400V, Largura de 600MBPS, Altura de 400V, Profundidade de 680MMHG e Volume de 163MM\n",
    "\n",
    "    #### Exemplo 2:\n",
    "    #### Descrição original:\n",
    "    Geladeira Industrial Capacidade 500L Temperatura mínima -20°C Temperatura máxima 10°C Consumo energético 200W Dimensões 180X70X60CM Peso 80KG\n",
    "\n",
    "    #### Variações:\n",
    "    1. Geladeira Industrial com Capacidade de 500 Litros, Temperatura Mínima de -20 Graus Celsius e Máxima de 10 Graus Celsius, Consumo Energético de 200 Watts, Dimensões de 180 por 70 por 60 Centímetros e Peso de 80 Quilogramas\n",
    "    2. Refrigerador Industrial com Volume de 500L, suportando Temperaturas de -20°C a 10°C, Consumo de Energia de 200W, Medidas de 180X70X60CM e Peso de 80KG\n",
    "    3. Unidade de Refrigeração Industrial com Capacidade de 500L, operando entre -20°C e 10°C, Consumo de 200W, Dimensões de 180X70X60CM e Massa de 80KG\n",
    "\n",
    "    ### Agora, gere um prompt de resposta sem nenhuma mensagem de introdução, apenas apresente uma resposta no seguinte formato:\n",
    "    1.\n",
    "    Gere apenas uma descrição contendo sinônimos para o seguinte produto:\n",
    "    \"\"\"\n",
    "\n",
    "    data = []\n",
    "\n",
    "    for index, row in df.iterrows():\n",
    "        product_description = f\"####\\n{row['query']}\\n\\n####:\\n\"\n",
    "        completion = client.chat.completions.create(\n",
    "            model=\"llama3-70b-8192\",\n",
    "            messages=[\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": instructions + product_description\n",
    "                }\n",
    "            ],\n",
    "            temperature=0.9,\n",
    "            max_tokens=2048,\n",
    "            top_p=0.95,\n",
    "            stream=False,\n",
    "            stop=None,\n",
    "        )\n",
    "        \n",
    "        generated_variation = completion.choices[0].message.content.strip()\n",
    "        print(generated_variation)\n",
    "\n",
    "        data.append({\n",
    "            \"query\": row['query'],\n",
    "            \"com_sinonimos\": generated_variation\n",
    "        })\n",
    "\n",
    "        data.append({\n",
    "            \"query\": row['query'],\n",
    "            \"com_sinonimos\": generated_variation\n",
    "        })\n",
    "        \n",
    "        print(generated_variation)\n",
    "    df_sinonimo = pd.DataFrame(data)\n",
    "    \n",
    "    return df_sinonimo\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_catmat_llm = pd.read_csv('queries_catmat_ruidos_a.csv')\n",
    "df_catmat_llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sinonimos = generate_synonym_descriptions(df_catmat_llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sinonimos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def tratar_descricao(df, coluna):\n",
    "    def remover_prefixo(descricao):\n",
    "        if descricao.startswith(\"1. \"):\n",
    "            return descricao[3:]\n",
    "        return descricao\n",
    "\n",
    "    df[coluna] = df[coluna].apply(remover_prefixo)\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sinonimos_catm_llam = tratar_descricao(df_sinonimos, 'com_sinonimos')\n",
    "df_sinonimos_catm_llam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sinonimos_catm_llam.to_csv('sinonimos_llama.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dv_relev_novo_reord = df_termos_reord.reset_index(drop =True)\n",
    "dv_relev_novo_erro = df_erros_dig.reset_index(drop =True)\n",
    "dv_relev_sin = df_sinonimos_catm_llam.reset_index(drop =True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dv_relev_sin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_queries_relv['sinonimos'] = dv_relev_sin['com_sinonimos']\n",
    "df_catmat_llm['sinonimos'] = dv_relev_sin['com_sinonimos']\n",
    "df_catmat_llm.to_csv('queries_catmat_ruidos_ctm.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_queries_relv['erros_digitacao'] = dv_relev_novo_erro['erros_digitacao']\n",
    "df_queries_relv['reordenados'] = dv_relev_novo_reord['reordenados']\n",
    "df_queries_relv['sinonimos'] = dv_relev_sin['com_sinonimos']\n",
    "df_queries_relv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "erros_a = df_queries_relv['query']\n",
    "erros_a.to_csv('erros_a.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_queries_relv.to_csv('queries_catmat_ruidos_ctm.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_queries_relv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_queries_relv.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_column_query = df_queries_relv['query']\n",
    "df_column_query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_column_query.drop_duplicates(inplace=True)\n",
    "df_column_query.to_csv('/home/melquicarvalho/Documentos/tcctest/queries_relv_descricoes.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices = es.cat.indices(h='index', format='json')\n",
    "nomes_indices = [indice['index'] for indice in indices]\n",
    "nomes_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = \"mat_semaanc\"\n",
    "index_info = es.cat.indices(index=index, v=True)\n",
    "index_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from requests.auth import HTTPBasicAuth\n",
    "\n",
    "logging.basicConfig(filename= 'experiment_relv_catmat.log',\n",
    "    filemode='a',\n",
    "    format='%(asctime)s,%(msecs)d %(name)s %(levelname)s %(message)s',\n",
    "    datefmt='%H:%M:%S',\n",
    "    level=logging.DEBUG)\n",
    "    \n",
    "def run(file):\n",
    "    total_time = 0\n",
    "    processed_queries = 0\n",
    "    experiment_queries = pd.read_csv(file)\n",
    "    for key, value in experiment_queries.iterrows():\n",
    "        print('key', key)\n",
    "        # print('value', value)\n",
    "\n",
    "        logging.info(f'Iniciando processamento da query {value[\"query\"]} com pagina de tamanho {value[\"page_size\"]}')\n",
    "        try:\n",
    "            for i in ['query', 'sinonimos', 'erros_digitacao', 'reordenados']:\n",
    "                match_query = ''\n",
    "                query = ''        \n",
    "                if(value['abordagem']) == 'semantico':\n",
    "                    index_consulta = 'mat_semaanc'\n",
    "                    match_query = modelL6V2.encode(value[i])\n",
    "                    \n",
    "                    query = {\n",
    "                        \"query\": {\n",
    "                            \"script_score\": {\n",
    "                                \"query\": {\"match_all\": {}},\n",
    "                                \"script\": {\n",
    "                                    \"source\": \"cosineSimilarity(params.query_vector, 'all_mini_base_vector') + 1.0\",\n",
    "                                    \"params\": {\"query_vector\": match_query.tolist()}\n",
    "                                }\n",
    "                            }\n",
    "                        },\"size\": 100\n",
    "                    }\n",
    "                else:\n",
    "                    index_consulta = 'matlex'\n",
    "                    match_query = value[i]\n",
    "                    \n",
    "                    query = {\n",
    "                        \"query\": {\n",
    "                            \"match\": {\n",
    "                                \"descricao_produto\": match_query\n",
    "                            }\n",
    "                        },\"size\": 100\n",
    "                    }              \n",
    "        \n",
    "                sucess = False\n",
    "                retry_times = 0\n",
    "                # print('abordagem', value['abordagem'])\n",
    "\n",
    "                while(not sucess and retry_times < 10):\n",
    "\n",
    "                    start_time = time.perf_counter()\n",
    "                    result = requests.get('http://150.165.75.163:9202/' + index_consulta + '/_search', json=query, headers={'Content-type': 'application/json'},\n",
    "                                          auth=HTTPBasicAuth('elastic', 'teste123'))\n",
    "                    time_elapsed = time.perf_counter() - start_time\n",
    "                    total_time += time_elapsed\n",
    "                    # print('result', result)\n",
    "\n",
    "                    if result.status_code == requests.codes.ok:\n",
    "                        sucess = True\n",
    "                    else:\n",
    "                        retry_times += 1\n",
    "                    # print('result', result)\n",
    "                processed_queries += 1\n",
    "                result_query = 'result_' + i \n",
    "                # print('query', result.content)\n",
    "\n",
    "                experiment_queries.loc[key, result_query] = result.content.decode('utf-8')\n",
    "\n",
    "                remaining_time = (total_time / processed_queries) * (len(experiment_queries) - processed_queries)\n",
    "\n",
    "                logging.info(f'A query levou {time_elapsed:.6f}s')\n",
    "                logging.info(f'Faltam aproximadamente {int(remaining_time // 3600)}h, {int((remaining_time % 3600) // 60)}m e {math.floor(remaining_time % 60)}s')\n",
    "            \n",
    "        except Exception as error:\n",
    "            logging.error(f'Erro na consulta: {error}')\n",
    "    experiment_queries.to_csv('queries_result_catmat_llam.csv')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run('/mnt/DADOS/melqui/busca-smt/queries_catmat_ruidos_a.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = pd.read_csv('queries_result_catmat_llam.csv')\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results[results['sinonimos'].isnull()]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in ['query', 'sinonimos', 'erros_digitacao', 'reordenados']:\n",
    "    result_query = 'result_' + i\n",
    "    print(result_query)\n",
    "    results[result_query] = results.apply(lambda row: json.loads(row[result_query]), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx, row in results.iterrows():\n",
    "    result = row['result_query']\n",
    "    if 'hits' in result:\n",
    "        print(result['hits']['hits'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def precision(results, expected):\n",
    "    hits = 0\n",
    "    lista = [x[0] for x in expected]\n",
    "    for result in results:\n",
    "        if result['id_material'] in lista:\n",
    "            hits += 1\n",
    "    if len(results) == 0:\n",
    "        return 0\n",
    "    precision_value = hits / len(results)\n",
    "    return precision_value\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recall(results, expected, abordagem, size_q, query_exp, cont):\n",
    "    expected_ids = set(map(lambda x: x[0], expected))\n",
    "    returned_ids = set(map(lambda x: x['id_material'], results))\n",
    "    intersection = expected_ids.intersection(returned_ids)\n",
    "    recaal = len(intersection) / len(expected) if len(expected) > 0 else 0\n",
    "    return recaal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('pt_core_news_sm')\n",
    "def custom_preprocessor(text):\n",
    "    doc = nlp(text)\n",
    "    lemmatized_words = [token.lemma_ for token in doc]\n",
    "    return ' '.join(lemmatized_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_vectorizer = TfidfVectorizer(preprocessor=custom_preprocessor, lowercase=True, strip_accents='unicode', ngram_range=(1, 3))\n",
    "tfidf_vectorizer.fit(df_index_catmat['MATERIAL_TEXT'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_similar(desc1, desc2):\n",
    "    vectors = tfidf_vectorizer.transform([desc1, desc2])\n",
    "\n",
    "    vector_emb1 = modelL6V2.encode(desc1) \n",
    "    vector_emb1 = vector_emb1.reshape(1, -1)\n",
    "\n",
    "    vector_emb2 = modelL6V2.encode(desc2)\n",
    "    vector_emb2 = vector_emb2.reshape(1, -1)\n",
    "    \n",
    "    cosine_dist_emb = cosine_similarity(vector_emb1, vector_emb2)\n",
    "    cosine_dist_tf_idf = cosine_similarity(vectors[0], vectors[1])\n",
    "    \n",
    "    value_cosi =  max(cosine_dist_tf_idf, cosine_dist_emb)\n",
    "    if value_cosi > 0.7:\n",
    "        return True\n",
    "    else:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ndcg(results, expected, query):\n",
    "    annoted_results = []\n",
    "    for result in results:\n",
    "        r_copy = copy.deepcopy(result)\n",
    "        filtered = list(filter(lambda x: x[0] == result['id_material'], expected))\n",
    "        r_copy['relevance'] = filtered[0][1] if len(filtered) > 0 else 1 if is_similar(result['material_text'], query) else 0\n",
    "        annoted_results.append(r_copy)\n",
    "    \n",
    "    dcg = 0\n",
    "    idcg = 0\n",
    "    sorted_results = sorted(annoted_results, key=lambda x: x['relevance'], reverse=True)\n",
    "    for i in range(len(results)):\n",
    "        dcg += (2**annoted_results[i]['relevance'] - 1) / math.log2(i + 2)\n",
    "        idcg += (2**sorted_results[i]['relevance'] - 1) / math.log2(i + 2)\n",
    "    if idcg == 0: return 0\n",
    "    return dcg / idcg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_rows', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_geral = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "ruidos = ['query', 'sinonimos', 'erros_digitacao', 'reordenados']\n",
    "queries_df = pd.read_csv('sample_catmat_test.csv')\n",
    "queries_df['relevantes'] = queries_df.apply(lambda row: ast.literal_eval(row['relevantes']), axis=1)\n",
    "\n",
    "cont= 0\n",
    "processed = 0\n",
    "size = len(results)\n",
    "# size_queries = [1,2,3,5,10,20,25,30,40,50]\n",
    "size_queries = [30]\n",
    "\n",
    "for size_q in size_queries:\n",
    "    results_metricas = pd.DataFrame()\n",
    "    results_metricas['abordagem'] = None\n",
    "    results_metricas['size_page'] = None\n",
    "    for i in ruidos:\n",
    "        results_metricas['precision_' + i] = None\n",
    "        results_metricas['recall_' + i] = None\n",
    "        results_metricas['f1-score_' + i] = None\n",
    "        results_metricas['ndcg_' + i] = None\n",
    "    for idx, row in results.iterrows():\n",
    "        results_metricas.loc[idx, 'size_page'] = size_q\n",
    "                \n",
    "        processed += 1\n",
    "        abord = results.loc[idx, 'abordagem']\n",
    "        print('abord', abord)\n",
    "        expected = queries_df.loc[row['idx'], 'relevantes']\n",
    "        abordagem = results.loc[idx, 'abordagem']\n",
    "        results_metricas.loc[idx, 'abordagem'] = abordagem\n",
    "        if processed % 500 == 0:\n",
    "            print(f'{processed} de {size} ({(processed/size)*100:.2f}%)')\n",
    "        for i in ruidos:\n",
    "            result = row['result_' + i]\n",
    "            items = result['hits']['hits']\n",
    "            query_expected = queries_df.loc[row['idx'], 'MATERIAL_TEXT']\n",
    "            print('query', query_expected)\n",
    "            if items is not None:\n",
    "                conteudos = [objeto['_source'] for objeto in items]\n",
    "                precision_column = 'precision_' + i\n",
    "                recall_column = 'recall_' + i\n",
    "                print('conteudo', conteudos)\n",
    "                print('expected', expected)\n",
    "                conteudosSlice = conteudos[:size_q]\n",
    "\n",
    "                results_metricas.loc[idx, 'precision_' + i] = precision(conteudosSlice, expected)\n",
    "                results_metricas.loc[idx, 'recall_' + i] = recall(conteudosSlice, expected, abordagem, size_q, query_expected, cont)\n",
    "                results_metricas.loc[idx, 'f1-score_' + i] = (2 * results_metricas.loc[idx, precision_column] * results_metricas.loc[idx, recall_column]) / (results_metricas.loc[idx, precision_column] + results_metricas.loc[idx, recall_column]) if results_metricas.loc[idx, precision_column] + results_metricas.loc[idx, recall_column] > 0 else 0\n",
    "                results_metricas.loc[idx, 'ndcg_' + i] = ndcg(conteudosSlice, expected, query_expected)\n",
    "    df_geral.append(results_metricas)\n",
    "df_por_sizes = pd.concat(df_geral)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Métricas por tamanho da page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_por_sizes = pd.read_csv('df_por_sizes.csv')\n",
    "df_por_sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_por_sizes.to_csv('df_por_sizes_catm.csv')\n",
    "df_por_sizes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# is similar\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped = df_por_sizes.groupby(['size_page', 'abordagem'], as_index=False).agg({'ndcg_query': 'mean', 'precision_query': 'mean', 'f1-score_query': 'mean', 'recall_query': 'mean'})\n",
    "grouped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped = df_por_sizes.groupby(['size_page', 'abordagem'], as_index=False).agg({'ndcg_erros_digitacao': 'mean', 'precision_erros_digitacao': 'mean', 'f1-score_erros_digitacao': 'mean', 'recall_erros_digitacao': 'mean'})\n",
    "grouped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped = df_por_sizes.groupby(['size_page', 'abordagem'], as_index=False).agg({'ndcg_reordenados': 'mean', 'precision_reordenados': 'mean', 'f1-score_reordenados': 'mean', 'recall_reordenados': 'mean'})\n",
    "grouped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped = df_por_sizes.groupby(['size_page', 'abordagem'], as_index=False).agg({'ndcg_sinonimos': 'mean', 'precision_sinonimos': 'mean', 'f1-score_sinonimos': 'mean', 'recall_sinonimos': 'mean'})\n",
    "grouped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_metricas.groupby(['abordagem']).agg(\n",
    "    {'ndcg_reordenados': ['mean', 'std'], 'precision_reordenados': ['mean', 'std'], 'f1-score_reordenados': ['mean', 'std'], 'recall_reordenados': ['mean', 'std']})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed = 0\n",
    "size = len(results)\n",
    "teste_2 = pd.DataFrame()\n",
    "for idx, row in results.iterrows():\n",
    "    for i in ruidos:\n",
    "        results_ruidos = row['result_' + i]\n",
    "        items = results_ruidos['hits']['hits']\n",
    "        conteudos = [objeto['_source'] for objeto in items]\n",
    "        processed += 1\n",
    "        if processed % 100 == 0:\n",
    "            print(f'{processed} de {size} ({(processed/size)*100:.2f}%)')\n",
    "        \n",
    "        query_expected = queries_df.loc[row['idx'], 'DESCRICAO_PRODUTO']\n",
    "        expected = queries_df.loc[row['idx'], 'relevantes']\n",
    "        annoted_results = []\n",
    "        for result in conteudos:\n",
    "            r_copy = copy.deepcopy(result)\n",
    "            filtered = list(filter(lambda x: x[0] == result['id'], expected))\n",
    "            r_copy['relevance'] = filtered[0][1] if len(filtered) > 0 else 1 if is_similar(result['descricao_produto'], query_expected) else 0\n",
    "            annoted_results.append(r_copy)\n",
    "        teste_2['result_' + i] = annoted_results\n",
    "teste_2    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_100 = results[results['page_size'] == 100]\n",
    "\n",
    "processed = 0\n",
    "size = len(results_100)\n",
    "for idx, row in results_100.iterrows():\n",
    "    processed += 1\n",
    "    if processed % 100 == 0:\n",
    "        print(f'{processed} de {size} ({(processed/size)*100:.2f}%)')\n",
    "    items = result['hits']['hits']\n",
    "    conteudos = [objeto['_source'] for objeto in items]\n",
    "            \n",
    "    expected = queries_df.loc[row['idx'], 'relevantes']\n",
    "    annoted_results = []\n",
    "    for result in conteudos:\n",
    "        r_copy = copy.deepcopy(result)\n",
    "        filtered = list(filter(lambda x: x[0] == result['id'], expected))\n",
    "        r_copy['relevance'] = filtered[0][1] if len(filtered) > 0 else 1 if is_similar(result['descricao'], query) else 0\n",
    "        annoted_results.append(r_copy)\n",
    "    row['result'] = annoted_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_por_sizes = pd.read_csv('df_por_sizes.csv')\n",
    "df_por_sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped = df_por_sizes.groupby(['size_page', 'abordagem'], as_index=False).agg({\n",
    "    'ndcg_erros_digitacao': 'mean', 'precision_erros_digitacao': 'mean', 'f1-score_erros_digitacao': 'mean', 'recall_erros_digitacao': 'mean',\n",
    "    'ndcg_query': 'mean', 'precision_query': 'mean', 'f1-score_query': 'mean', 'recall_query': 'mean',\n",
    "    'ndcg_sinonimos': 'mean', 'precision_sinonimos': 'mean', 'f1-score_sinonimos': 'mean', 'recall_sinonimos': 'mean',\n",
    "    'ndcg_reordenados': 'mean', 'precision_reordenados': 'mean', 'f1-score_reordenados': 'mean', 'recall_reordenados': 'mean'\n",
    "    })\n",
    "grouped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "lexico = grouped[grouped['abordagem'] == 'lexico']   \n",
    "\n",
    "semantico = grouped[grouped['abordagem'] == 'semantico']   \n",
    "_, ax = plt.subplots(2, 2, figsize=(15,10))\n",
    "ax[0,0].set_title('reordenados')\n",
    "ax[0,0].set_xlabel('Tamanho da página')\n",
    "ax[0,0].set_ylabel('Pontuação de NDCG')\n",
    "ax[0,0].plot(lexico['size_page'], lexico['precision_reordenados'], label='lexico')\n",
    "ax[0,0].plot(semantico['size_page'], semantico['precision_reordenados'], label='semantico', color='red', linewidth=1.0)\n",
    "ax[0,0].legend()\n",
    "\n",
    "ax[0,1].set_title('erros de digitação')\n",
    "ax[0,1].set_xlabel('Tamanho da página')\n",
    "ax[0,1].set_ylabel('Pontuação de NDCG')\n",
    "ax[0,1].plot(lexico['size_page'], lexico['precision_erros_digitacao'], label='lexico')\n",
    "ax[0,1].plot(semantico['size_page'], semantico['precision_erros_digitacao'], label='semantico', color='red', linewidth=1.0)\n",
    "ax[0,1].legend()\n",
    "\n",
    "ax[1,0].set_title('sinonimos')\n",
    "ax[1,0].set_xlabel('Tamanho da página')\n",
    "ax[1,0].set_ylabel('Pontuação de NDCG')\n",
    "ax[1,0].plot(lexico['size_page'], lexico['precision_sinonimos'], label='lexico')\n",
    "ax[1,0].plot(semantico['size_page'], semantico['precision_sinonimos'], label='semantico', color='red', linewidth=1.0)\n",
    "ax[1,0].legend()\n",
    "\n",
    "ax[1,1].set_title('query')\n",
    "ax[1,1].set_xlabel('Tamanho da página')\n",
    "ax[1,1].set_ylabel('Pontuação de NDCG')\n",
    "ax[1,1].plot(lexico['size_page'], lexico['precision_query'], label='lexico')\n",
    "ax[1,1].plot(semantico['size_page'], semantico['precision_query'], label='semantico', color='red', linewidth=1.0)\n",
    "ax[1,1].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.gcf().savefig('precision_por_size.png', dpi=300)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "grouped = df_por_sizes.groupby(['size_page', 'abordagem'], as_index=False).agg({'ndcg_query': 'mean', 'precision_query': 'mean', 'f1-score_query': 'mean', 'recall_query': 'mean'})\n",
    "grouped\n",
    "for name, group in grouped:\n",
    "    plt.plot(group['precision_query'], group['size_page'], marker='o', label=group['abordagem'])\n",
    "\n",
    "plt.title('Relação entre Pontuação e Tamanho por Tipo de Teste')\n",
    "plt.xlabel('Pontuação')\n",
    "plt.ylabel('Tamanho')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "elastic_data = grouped[grouped['sgbd'] == 'elastic']\n",
    "elastic_semantic_data = grouped[grouped['sgbd'] == 'elastic_semantic']\n",
    "sql_server_data = grouped[grouped['sgbd'] == 'sql server']\n",
    "\n",
    "metrics = ['ndcg', 'precision', 'recall', 'f1-score']\n",
    "metric_titles = ['NDCG', 'Precision', 'Recall', 'F1-score']\n",
    "xa = ['a)', 'b)', 'c)', 'd)']\n",
    "\n",
    "plt.style.use('custom_charts.mplstyle')\n",
    "fig = plt.figure(figsize=(28, 16))\n",
    "\n",
    "for i, metric in enumerate(metrics):\n",
    "    ax = fig.add_subplot(2, 2, i+1)\n",
    "    x = range(len(elastic_data))\n",
    "\n",
    "    ax.plot(range(len(elastic_data)), elastic_data[metric].tolist(), '-o', label=f'Elasticsearch {metric}')\n",
    "    ax.plot(range(len(elastic_data)), elastic_semantic_data[metric].tolist(), '-o', label=f'Elasticsearch Semantic {metric}')\n",
    "    ax.plot(range(len(elastic_data)), sql_server_data[metric].tolist(), '-x', label=f'SQL Server {metric}')\n",
    "    ax.set_ylabel(xa[i] + ' ' + metric_titles[i])\n",
    "    ax.set_xticks(x)\n",
    "    ax.set_xlabel('k')\n",
    "    ax.legend()\n",
    "    ax.set_xticklabels(elastic_data['page_size'])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.figure(figsize=(20, 10))\n",
    "plt.plot(relevantes_by_p[relevantes_by_p['sgbd'] == 'elastic']['position'], relevantes_by_p[relevantes_by_p['sgbd'] == 'elastic']['prop'], label='Elasticsearch', color='blue')\n",
    "plt.plot(relevantes_by_p[relevantes_by_p['sgbd'] == 'elastic semantic']['position'], relevantes_by_p[relevantes_by_p['sgbd'] == 'elastic semantic']['prop'], label='Elasticsearch Semantic', color='red')\n",
    "plt.plot(relevantes_by_p[relevantes_by_p['sgbd'] == 'sql server']['position'], relevantes_by_p[relevantes_by_p['sgbd'] == 'sql server']['prop'], label='Sql Server', color='orange')\n",
    "plt.plot(relevantes_by_p[relevantes_by_p['sgbd'] == 'sql server']['position'], relevantes_by_p[relevantes_by_p['sgbd'] == 'sql server']['prop_gabarito'], label='Ideal', color='green')\n",
    "plt.xlabel('Posição nos resultados')\n",
    "plt.ylabel('Porcentagem de relevântes')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped = df_por_sizes.groupby(['size_page', 'abordagem'], as_index=False).agg({'ndcg_sinonimos': 'mean', 'precision_sinonimos': 'mean', 'f1-score_sinonimos': 'mean', 'recall_sinonimos': 'mean'})\n",
    "grouped"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
